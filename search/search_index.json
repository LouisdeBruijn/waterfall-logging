{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Waterfall-logging is a Python package to log (distinct) column counts in a DataFrame, export it as a Markdown table and plot a Waterfall statistics figure.</p> <p>It provides an implementation in Pandas <code>PandasWaterfall</code> and PySpark <code>SparkWaterfall</code>.</p>"},{"location":"#project-overview","title":"Project overview","text":"<p>The documentation consists of four separate parts</p> <ol> <li>Tutorials are learning-oriented</li> <li>How-To Guides are task-oriented</li> <li>Reference is the technical documentation</li> <li>Explanation is understanding-oriented</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p>"},{"location":"#how-to-install","title":"How to install","text":"<p>Install Waterfall-logging from PyPi</p> <pre><code>pip install waterfall-logging\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import pandas as pd\nfrom waterfall_logging.log import PandasWaterfall\n\nbicycle_rides = pd.DataFrame(data=[\n    ['Shimano', 'race', 28, '2023-02-13', 1],\n    ['Gazelle', 'comfort', 31, '2023-02-15', 1],\n    ['Shimano', 'race', 31, '2023-02-16', 2],\n    ['Batavia', 'comfort', 30, '2023-02-17', 3],\n], columns=['brand', 'ride_type', 'wheel_size', 'date', 'bike_id'])\n\nbicycle_rides_log = PandasWaterfall(table_name='rides', columns=['brand', 'ride_type', 'wheel_size'],\n    distinct_columns=['bike_id'])\nbicycle_rides_log.log(table=bicycle_rides, reason='Logging initial column values', configuration_flag='')\n\nbicycle_rides = bicycle_rides.loc[lambda row: row['wheel_size'] &gt; 30]\nbicycle_rides_log.log(table=bicycle_rides, reason='Remove small wheels',\n    configuration_flag='small_wheel=False')\n\nprint(bicycle_rides_log.to_markdown())\n\n| Table   |   brand |   \u0394 brand |   ride_type |   \u0394 ride_type |   wheel_size |   \u0394 wheel_size |   bike_id |   \u0394 bike_id |   Rows |   \u0394 Rows | Reason                        | Configurations flag   |\n|:--------|--------:|----------:|------------:|--------------:|-------------:|---------------:|----------:|------------:|-------:|---------:|:------------------------------|:----------------------|\n| rides   |       4 |         0 |           4 |             0 |            4 |              0 |         3 |           0 |      4 |        0 | Logging initial column values |                       |\n| rides   |       2 |        -2 |           2 |            -2 |            2 |             -2 |         2 |          -1 |      2 |       -2 | Remove small wheels           | small_wheel=False     |\n</code></pre>"},{"location":"references/","title":"References","text":"<p>When doing any data-science project, an important part is being able to scope your data to a suitable dataset for validating your machine learning model.</p> <p>This package helps you understand what happens to your data when such scoping or filtering steps are enabled and can be beneficial during the model development stage, as well as during monitoring.</p>"},{"location":"references/#acknowledgements","title":"Acknowledgements","text":"<p>Many thanks to Paul Zhutovsky for our close collaboration and several improvements to the <code>SparkWaterfall</code> implementation.</p>"},{"location":"explanations/context_manager/","title":"Context manager","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of <code>waterfall-logging.context_manager</code>.</p>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.waterfall","title":"waterfall","text":"<pre><code>waterfall(log: Waterfall, variable_names: List[str], markdown_kwargs: Optional[Dict] = None)\n</code></pre> <p>Waterfall context manager decorator.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Waterfall</code> <p>Waterfall object</p> required <code>variable_names</code> <code>List</code> <p>array of variables names to log waterfall</p> required <code>markdown_kwargs</code> <code>dict</code> <p>keyword arguments to save the markdown with to_markdown(**markdown_kwargs)</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from waterfall_logging.log import PandasWaterfall\n&gt;&gt;&gt; from waterfall_logging.context_manager import waterfall\n&gt;&gt;&gt; waterfall_log = PandasWaterfall(table_name='context_manager_example', columns=['col1'])\n&gt;&gt;&gt; @waterfall(log=waterfall_log, variable_names=['table'], markdown_kwargs={'buf': 'ctx_manager_example.md'})\n... def main():\n...     table = pd.DataFrame({'col1': [0, np.nan, 1, 2, 3], 'col2': [3, 4, np.nan, 5, 6]})\n...     table.dropna(axis=0)\n...\n&gt;&gt;&gt; main()\n&gt;&gt;&gt; print(waterfall_log.to_markdown())\n| Table                   |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason   | Configurations flag   |\n|:------------------------|-------:|---------:|-------:|---------:|:---------|:----------------------|\n| context_manager_example |      5 |        0 |      5 |        0 |          |                       |\n&gt;&gt;&gt; print(\"The `waterfall_log` is also saved in the file `ctx_manager_example.md`!\")\n</code></pre> <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>decorator object</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def waterfall(log: Waterfall, variable_names: List[str], markdown_kwargs: Optional[Dict] = None):\n\"\"\"Waterfall context manager decorator.\n\n    Args:\n        log (Waterfall): Waterfall object\n        variable_names (List): array of variables names to log waterfall\n        markdown_kwargs (dict): keyword arguments to save the markdown with to_markdown(**markdown_kwargs)\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from waterfall_logging.log import PandasWaterfall\n        &gt;&gt;&gt; from waterfall_logging.context_manager import waterfall\n        &gt;&gt;&gt; waterfall_log = PandasWaterfall(table_name='context_manager_example', columns=['col1'])\n        &gt;&gt;&gt; @waterfall(log=waterfall_log, variable_names=['table'], markdown_kwargs={'buf': 'ctx_manager_example.md'})\n        ... def main():\n        ...     table = pd.DataFrame({'col1': [0, np.nan, 1, 2, 3], 'col2': [3, 4, np.nan, 5, 6]})\n        ...     table.dropna(axis=0)\n        ...\n        &gt;&gt;&gt; main()\n        &gt;&gt;&gt; print(waterfall_log.to_markdown())\n        | Table                   |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason   | Configurations flag   |\n        |:------------------------|-------:|---------:|-------:|---------:|:---------|:----------------------|\n        | context_manager_example |      5 |        0 |      5 |        0 |          |                       |\n        &gt;&gt;&gt; print(\"The `waterfall_log` is also saved in the file `ctx_manager_example.md`!\")\n\n\n    Returns:\n        decorator (Callable): decorator object\n\n    \"\"\"\n    markdown_kwargs = {} if not markdown_kwargs else markdown_kwargs\n\n    def decorator_waterfall(method: Callable):\n\"\"\"\"\"\"\n\n        @functools.wraps(method)\n        def wrapper_waterfall(*args, **kwargs):\n\"\"\"\"\"\"\n            with WaterfallContext(log, method.__name__, variable_names, markdown_kwargs):\n                return_value = method(*args, **kwargs)\n            return return_value\n\n        return wrapper_waterfall\n\n    return decorator_waterfall\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext","title":"WaterfallContext","text":"<p>Waterfall context to trace any function calls inside the context.</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>class WaterfallContext:\n\"\"\"Waterfall context to trace any function calls inside the context.\"\"\"\n\n    def __init__(self, log: Waterfall, name: str, variable_names: List[str], markdown_kwargs: Optional[Dict] = None):\n\"\"\"\n\n        Args:\n            log (Waterfall): waterfall logging object\n            name (str): name of the function that is to de debugged\n            variable_names (List): array of variables names to log waterfall\n            markdown_kwargs (dict): keyword arguments to save the markdown with to_markdown(**markdown_kwargs)\n\n        \"\"\"\n        self.name = name\n        self.variable_names = variable_names\n        self.log = log\n        self.markdown_kwargs = {} if not markdown_kwargs else markdown_kwargs\n\n        self._df = None\n\n    def __enter__(self):\n\"\"\"Enter the trace.\"\"\"\n        sys.settrace(self.trace_calls)\n\n    def __exit__(self, *args, **kwargs):\n\"\"\"Exit the trace and export Waterfall object to Markdown.\"\"\"\n        self.log.to_markdown(**self.markdown_kwargs)\n        sys.settrace = None\n\n    def trace_calls(self, frame: types.FrameType, event, arg: Any):\n\"\"\"\n\n        Args:\n            frame (types.FrameType): a tracing frame\n            event (str): a tracing event\n            arg (Any): a tracing argument\n\n        Returns:\n            traced_lines (types.MethodType): traced lines\n\n        \"\"\"\n        if event != \"call\":\n            # only trace our call to the decorated function\n            return\n        elif frame.f_code.co_name != self.name:\n            # return the trace function to use when you go into that function call\n            return\n\n        traced_lines = self.trace_lines\n        return traced_lines\n\n    def trace_lines(self, frame: types.FrameType, event: str, arg: Any) -&gt; None:\n\"\"\"\n\n        Args:\n            frame (types.FrameType): a tracing frame\n            event (str): a tracing event\n            arg (Any): a tracing argument\n\n        Returns:\n            None\n\n        \"\"\"\n        if event not in [\"line\", \"return\"]:\n            return\n\n        local_vars = frame.f_locals\n\n        for var_name, var_value in local_vars.items():\n            if var_name in self.variable_names:\n                df = local_vars[var_name]\n                if df is not self._df:\n                    self.log.log(df)\n                    self._df = df\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Enter the trace.</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def __enter__(self):\n\"\"\"Enter the trace.\"\"\"\n    sys.settrace(self.trace_calls)\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext.__exit__","title":"__exit__","text":"<pre><code>__exit__(*args, **kwargs)\n</code></pre> <p>Exit the trace and export Waterfall object to Markdown.</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def __exit__(self, *args, **kwargs):\n\"\"\"Exit the trace and export Waterfall object to Markdown.\"\"\"\n    self.log.to_markdown(**self.markdown_kwargs)\n    sys.settrace = None\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext.__init__","title":"__init__","text":"<pre><code>__init__(log: Waterfall, name: str, variable_names: List[str], markdown_kwargs: Optional[Dict] = None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Waterfall</code> <p>waterfall logging object</p> required <code>name</code> <code>str</code> <p>name of the function that is to de debugged</p> required <code>variable_names</code> <code>List</code> <p>array of variables names to log waterfall</p> required <code>markdown_kwargs</code> <code>dict</code> <p>keyword arguments to save the markdown with to_markdown(**markdown_kwargs)</p> <code>None</code> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def __init__(self, log: Waterfall, name: str, variable_names: List[str], markdown_kwargs: Optional[Dict] = None):\n\"\"\"\n\n    Args:\n        log (Waterfall): waterfall logging object\n        name (str): name of the function that is to de debugged\n        variable_names (List): array of variables names to log waterfall\n        markdown_kwargs (dict): keyword arguments to save the markdown with to_markdown(**markdown_kwargs)\n\n    \"\"\"\n    self.name = name\n    self.variable_names = variable_names\n    self.log = log\n    self.markdown_kwargs = {} if not markdown_kwargs else markdown_kwargs\n\n    self._df = None\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext.trace_calls","title":"trace_calls","text":"<pre><code>trace_calls(frame: types.FrameType, event, arg: Any)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>types.FrameType</code> <p>a tracing frame</p> required <code>event</code> <code>str</code> <p>a tracing event</p> required <code>arg</code> <code>Any</code> <p>a tracing argument</p> required <p>Returns:</p> Name Type Description <code>traced_lines</code> <code>types.MethodType</code> <p>traced lines</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def trace_calls(self, frame: types.FrameType, event, arg: Any):\n\"\"\"\n\n    Args:\n        frame (types.FrameType): a tracing frame\n        event (str): a tracing event\n        arg (Any): a tracing argument\n\n    Returns:\n        traced_lines (types.MethodType): traced lines\n\n    \"\"\"\n    if event != \"call\":\n        # only trace our call to the decorated function\n        return\n    elif frame.f_code.co_name != self.name:\n        # return the trace function to use when you go into that function call\n        return\n\n    traced_lines = self.trace_lines\n    return traced_lines\n</code></pre>"},{"location":"explanations/context_manager/#waterfall_logging.context_manager.WaterfallContext.trace_lines","title":"trace_lines","text":"<pre><code>trace_lines(frame: types.FrameType, event: str, arg: Any) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>types.FrameType</code> <p>a tracing frame</p> required <code>event</code> <code>str</code> <p>a tracing event</p> required <code>arg</code> <code>Any</code> <p>a tracing argument</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>waterfall_logging/context_manager.py</code> <pre><code>def trace_lines(self, frame: types.FrameType, event: str, arg: Any) -&gt; None:\n\"\"\"\n\n    Args:\n        frame (types.FrameType): a tracing frame\n        event (str): a tracing event\n        arg (Any): a tracing argument\n\n    Returns:\n        None\n\n    \"\"\"\n    if event not in [\"line\", \"return\"]:\n        return\n\n    local_vars = frame.f_locals\n\n    for var_name, var_value in local_vars.items():\n        if var_name in self.variable_names:\n            df = local_vars[var_name]\n            if df is not self._df:\n                self.log.log(df)\n                self._df = df\n</code></pre>"},{"location":"explanations/decorator/","title":"Decorator","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of <code>waterfall-logging.decorator</code>.</p>"},{"location":"explanations/decorator/#waterfall_logging.decorator.waterfall","title":"waterfall","text":"<pre><code>waterfall(log: Waterfall, reason: Optional[str] = None, configuration_flag: Optional[str] = None, table_name: Optional[str] = None) -&gt; Callable\n</code></pre> <p>Waterfall function decorator.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Waterfall</code> <p>Waterfall object</p> required <code>reason</code> <code>str</code> <p>Specifies reasoning for DataFrame filtering step</p> <code>None</code> <code>configuration_flag</code> <code>str</code> <p>Specifies configurations flag used for DataFrame filtering step</p> <code>None</code> <code>table_name</code> <code>str</code> <p>First column in table is the <code>table id</code> column that should contain the table name value</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from waterfall_logging.log import PandasWaterfall\n&gt;&gt;&gt; from waterfall_logging.decorator import waterfall\n&gt;&gt;&gt; waterfall_log = PandasWaterfall(table_name='decorator_example', columns=['col1'])\n&gt;&gt;&gt; @waterfall(log=waterfall_log, reason='dropping NaN')\n... def drop_na(table: pd.DataFrame):\n...     return table.dropna(axis=0)\n...\n&gt;&gt;&gt; df = drop_na(table=pd.DataFrame({'col1': [0, np.nan, 1, 2, 3], 'col2': [3, 4, np.nan, 5, 6]}))\n&gt;&gt;&gt; print(waterfall_log.to_markdown())\n| Table             |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason       | Configurations flag   |\n|:------------------|-------:|---------:|-------:|---------:|:-------------|:----------------------|\n| decorator_example |      3 |        0 |      3 |        0 | dropping NaN |                       |\n</code></pre> <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>decorator object</p> Source code in <code>waterfall_logging/decorator.py</code> <pre><code>def waterfall(\n    log: Waterfall,\n    reason: Optional[str] = None,\n    configuration_flag: Optional[str] = None,\n    table_name: Optional[str] = None,\n) -&gt; Callable:\n\"\"\"Waterfall function decorator.\n\n    Args:\n        log (Waterfall): Waterfall object\n        reason (str): Specifies reasoning for DataFrame filtering step\n        configuration_flag (str): Specifies configurations flag used for DataFrame filtering step\n        table_name (str): First column in table is the `table id` column that should contain the table name value\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from waterfall_logging.log import PandasWaterfall\n        &gt;&gt;&gt; from waterfall_logging.decorator import waterfall\n        &gt;&gt;&gt; waterfall_log = PandasWaterfall(table_name='decorator_example', columns=['col1'])\n        &gt;&gt;&gt; @waterfall(log=waterfall_log, reason='dropping NaN')\n        ... def drop_na(table: pd.DataFrame):\n        ...     return table.dropna(axis=0)\n        ...\n        &gt;&gt;&gt; df = drop_na(table=pd.DataFrame({'col1': [0, np.nan, 1, 2, 3], 'col2': [3, 4, np.nan, 5, 6]}))\n        &gt;&gt;&gt; print(waterfall_log.to_markdown())\n        | Table             |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason       | Configurations flag   |\n        |:------------------|-------:|---------:|-------:|---------:|:-------------|:----------------------|\n        | decorator_example |      3 |        0 |      3 |        0 | dropping NaN |                       |\n\n    Returns:\n        decorator (Callable): decorator object\n\n    \"\"\"\n\n    def decorator_waterfall(method: Callable):\n\"\"\"\"\"\"\n\n        @functools.wraps(method)\n        def wrapper_waterfall(*args, **kwargs):\n\"\"\"\"\"\"\n            table = method(*args, **kwargs)\n            log.log(table, reason, configuration_flag, table_name)\n            return table\n\n        return wrapper_waterfall\n\n    return decorator_waterfall\n</code></pre>"},{"location":"explanations/log/","title":"Log","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of <code>waterfall-logging.log</code>.</p> <p>Both <code>PandasWaterfall</code> and <code>SparkWaterfall</code> inherit from the <code>Waterfall</code> base class. Please refer to the methods available in both implementations in the <code>Waterfall</code> section below.</p>"},{"location":"explanations/log/#waterfall_logging.log.PandasWaterfall","title":"<code>PandasWaterfall</code>","text":"<p>         Bases: <code>Waterfall</code></p> <p>Logs a Pandas DataFrame on each filtering step in waterfall fashion.</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>class PandasWaterfall(Waterfall):\n\"\"\"Logs a Pandas DataFrame on each filtering step in waterfall fashion.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _count_or_distinct(self, table: pd.DataFrame) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` including NaNs.\"\"\"\n        table_length = len(table)\n\n        input_columns = [\n            table[c].nunique(dropna=False) if c in self.distinct_columns else table_length for c in self._input_columns\n        ]\n\n        if self.row_count_column:\n            input_columns.append(table_length)\n\n        return input_columns\n\n    def _count_or_distinct_dropna(self, table: pd.DataFrame) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` excluding NaNs.\"\"\"\n        input_columns = [\n            table[c].nunique(dropna=True) if c in self.distinct_columns else table[c].count()\n            for c in self._input_columns\n        ]\n\n        if self.row_count_column:\n            input_columns.append(len(table))\n\n        return input_columns\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.SparkWaterfall","title":"<code>SparkWaterfall</code>","text":"<p>         Bases: <code>Waterfall</code></p> <p>Logs a PySpark DataFrame on each filtering step in waterfall fashion.</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>class SparkWaterfall(Waterfall):\n\"\"\"Logs a PySpark DataFrame on each filtering step in waterfall fashion.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _count_or_distinct(self, table: pyspark.sql.DataFrame) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` including NaNs.\"\"\"\n        expressions = [F.count_distinct(F.col(col)).alias(col) for col in self.distinct_columns]\n        # `F.count_distinct` excludes None values in columns of StringType()\n        expressions.append(F.count(F.lit(1)).alias(\"row_count\"))\n\n        df = table.agg(*expressions).toPandas()\n        row_count = df.loc[0, \"row_count\"]\n\n        entries = [row_count] * len(self.columns) + [df.loc[0, col] for col in self.distinct_columns]\n\n        if self.row_count_column:\n            entries.append(row_count)\n\n        return entries\n\n    def _count_or_distinct_dropna(self, table: pyspark.sql.DataFrame) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` excluding NaNs.\"\"\"\n        count_expressions = [\n            F.count(\n                F.when(\n                    ~F.isnan(\n                        F.col(col),\n                    ),\n                    F.col(col),\n                ),\n            ).alias(col)\n            for col in self.columns\n        ]\n\n        distinct_expressions = [\n            F.count_distinct(\n                F.when(\n                    ~F.isnan(\n                        F.col(col),\n                    ),\n                    F.col(col),\n                ),\n            ).alias(col)\n            for col in self.distinct_columns\n        ]\n\n        expressions = count_expressions + distinct_expressions\n\n        if self.row_count_column:\n            expressions.append(F.count(F.lit(1)).alias(\"row_count\"))\n            df = table.agg(*expressions).toPandas()\n\n            return [df.loc[0, col] for col in self._input_columns] + [df.loc[0, \"row_count\"]]\n        else:\n            df = table.agg(*expressions).toPandas()\n            return [df.loc[0, col] for col in self._input_columns]\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall","title":"Waterfall","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Logs a table on each filtering step in waterfall fashion.</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>class Waterfall(abc.ABC):\n\"\"\"Logs a table on each filtering step in waterfall fashion.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str | None = None,\n        columns: List[str] | None = None,\n        distinct_columns: List[str] | None = None,\n        dropna: bool = False,\n        delta_prefix: str = \"\u0394 \",\n        row_count_column: str = \"Rows\",\n    ):\n\"\"\"\n\n        Args:\n            table_name (str): Specifies the name of the table to log\n            columns (Lists[str]): Specifies which columns to log\n            distinct_columns (List[str]): Specifies which distinct column values to log\n            delta_prefix (str): Prefix for column names with discrete difference (delta) with previous row\n            dropna (bool): Whether to exclude NaN in the row counts\n            row_count_column (str): Column name for an added column that counts rows in table\n\n        \"\"\"\n        self.table_name = table_name\n        self.columns = iterable_to_list(columns)\n        self.distinct_columns = iterable_to_list(distinct_columns)\n        self.delta_prefix = delta_prefix\n        self.dropna = dropna\n        self.row_count_column = row_count_column\n\n        self._input_columns = self.columns + self.distinct_columns\n        self._all_columns = self._input_columns.copy()\n        if self.row_count_column:\n            self._all_columns.append(self.row_count_column)\n        self._static_columns = [\"Table\", \"Reason\", \"Configurations flag\"]\n        self._log = None\n\n        distinct_overwrite = set(self.columns).intersection(set(self.distinct_columns))\n        if distinct_overwrite:\n            warnings.warn(\n                f\"Column names in `distinct_columns` overwrite names in `columns` with distinct counts: \"\n                f\"{distinct_overwrite}.\",\n            )\n\n    def _create_log(self) -&gt; pd.DataFrame:\n\"\"\"\"\"\"\n        columns_with_deltas = []\n        for column in self._all_columns:\n            columns_with_deltas += [column, f\"{self.delta_prefix}{column}\"]\n\n        column_names = [self._static_columns[0]] + columns_with_deltas + self._static_columns[1:]\n        return pd.DataFrame(columns=column_names)\n\n    def _count_entries(self, table) -&gt; List[int]:\n\"\"\"\"\"\"\n        if self.dropna:\n            return self._count_or_distinct_dropna(table)\n        else:\n            return self._count_or_distinct(table)\n\n    @abc.abstractmethod\n    def _count_or_distinct(self, table) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` including NaNs.\"\"\"\n\n    @abc.abstractmethod\n    def _count_or_distinct_dropna(self, table) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` excluding NaNs.\"\"\"\n\n    def log(\n        self,\n        table: pd.DataFrame | pyspark.sql.DataFrame,\n        reason: str | None = None,\n        configuration_flag: str | None = None,\n        table_name: str | None = None,\n    ) -&gt; None:\n\"\"\"Logs table (distinct) counts to logging DataFrame.\n\n        Args:\n            table (pd.DataFrame): DataFrame that the filtering is applied to\n            reason (str): Specifies reasoning for DataFrame filtering step\n            configuration_flag (str): Specifies configurations flag used for DataFrame filtering step\n            table_name (str): First column in table is the `table id` column that should contain the table name value\n\n        Examples:\n            &gt;&gt;&gt; waterfall = PandasWaterfall()\n            &gt;&gt;&gt; waterfall.log(table, reason='Filtered in-scope bicycles', configuration_flag='inscope=True',\n            ... table_name='sample_table')\n\n        Returns:\n            None\n\n        \"\"\"\n        table_name = table_name or self.table_name\n\n        if self._log is None:\n            self._log = self._create_log()\n\n        current_table = self._log.loc[self._log.iloc[:, 0] == table_name]\n        entries = self._count_entries(table)\n\n        prior_entries = (\n            entries.copy() if current_table.empty else current_table[self._all_columns].iloc[-1, :].to_list()\n        )\n\n        column_entries = []\n        for entry, prior_entry in zip(entries, prior_entries):\n            column_entries += [entry, entry - prior_entry]\n\n        calculated_columns = [table_name] + column_entries + [reason, configuration_flag]\n\n        self._log.loc[len(self._log)] = calculated_columns\n\n    def read_markdown(\n        self,\n        filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n        columns: List[str] | None = None,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n\"\"\"Reads table from Markdown file.\n\n        Args:\n            filepath_or_buffer (str, path object or file-like object):\n                Any valid string path is acceptable. The string could be a URL. Valid\n                URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n                expected. A local file could be: file://localhost/path/to/table.csv.\n                If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n                By file-like object, we refer to objects with a ``read()`` method, such as\n                a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n            columns (Lists[str]): Specifies which column to read in and log\n\n        Examples:\n            &gt;&gt;&gt; f = open('output/tests/read_markdown_table.md', 'r')\n            &gt;&gt;&gt; print(f.read())\n            | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag   |\n            |:-------|-------:|---------:|-------:|---------:|:--------|:----------------------|\n            | table1 |     50 |        0 |   2727 |        0 | initial | read_markdown         |\n            | table1 |    150 |      100 |   2827 |        0 | add-on  | read_markdown         |\n            | table1 |    250 |      100 |   2927 |      100 | extra   | read_markdown         |\n            &gt;&gt;&gt; waterfall = PandasWaterfall()\n            &gt;&gt;&gt; waterfall.read_markdown(\n            ...     filepath_or_buffer='output/tests/read_markdown_table.md',\n            ....    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n            ... )\n            &gt;&gt;&gt; print(waterfall._log)\n                Table  col1  \u0394 col1  Rows  \u0394 Rows   Reason Configurations flag\n            0  table1    50       0  2727       0  example       read_markdown\n            1  table1   150     100  2827       0  add-on        read_markdown\n            1  table1   250     100  2927       0  extra         read_markdown\n            &gt;&gt;&gt; print(type(waterfall._log))\n            &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n        Returns:\n            None\n\n        \"\"\"\n        self._log = (\n            pd.read_table(filepath_or_buffer, *args, **kwargs)\n            # strips trailing whitespaces in column names\n            # drops columns with all NA values\n            .rename(str.rstrip, axis=\"columns\").dropna(axis=1, how=\"all\")\n            # strips trailing whitespaces in column values\n            .apply(lambda row: row.str.rstrip() if row.dtype == object else row)\n        )\n        self._log = self._log[columns] if columns else self._log\n\n    def to_markdown(self, *args, index=False, **kwargs):\n\"\"\"Print DataFrame in Markdown-friendly format.\n\n        Args:\n            index (bool): Add index (row) labels\n\n        Examples:\n            &gt;&gt;&gt; waterfall = PandasWaterfall()\n            &gt;&gt;&gt; print(waterfall.to_markdown(index=True))\n            |    | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n            |---:|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n            |  0 | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n            |  1 | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n            &gt;&gt;&gt; print(waterfall.to_markdown(index=False))\n            | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n            |:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n            | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n            | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n\n        Returns:\n            (str): DataFrame in Markdown-friendly format\n\n        \"\"\"\n        return self._log.to_markdown(*args, index=index, **kwargs)\n\n    def plot(\n        self,\n        *args,\n        y_col: str = \"Rows\",\n        y_col_delta: str = \"\u0394 Rows\",\n        x_col: str = \"Reason\",\n        drop_zero_delta: bool = False,\n        **kwargs,\n    ) -&gt; go.Figure:\n\"\"\"Plots a logging DataFrame column in a waterfall chart.\n\n        Args:\n            y_col (str): Specifies column that contains absolute value counts for y-axis of plot\n            y_col_delta (str): Specifies column that contains delta value counts for y-axis of plot\n            x_col (str): Specifies column that contains the filtering explanation for x-axis of plot\n            drop_zero_delta (bool): Whether to remove rows for `y_col_delta` that contain zeros\n\n        Examples:\n            &gt;&gt;&gt; waterfall = PandasWaterfall()\n            &gt;&gt;&gt; fig = waterfall.plot(y_col='Rows', y_col_delta='\u0394 Rows', x_col='Reason',\n            ... textfont=dict(family='sans-serif', size=11),\n            ... connector={'line': {'color': 'rgba(0,0,0,0)'}},\n            ... totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n            ... )\n\n        Returns:\n            go.Figure: waterfall chart\n\n        \"\"\"\n        df = self._log.copy()\n        if drop_zero_delta:\n            df = df.iloc[1:]\n            indices = df[(df[y_col_delta] == 0)].index\n            df = df.drop(indices, inplace=False)\n\n        measure = [\"absolute\"] + [\"relative\"] * (df.shape[0] - 1) + [\"total\"]\n        x = df[x_col].to_list() + [\"Total\"]\n        y = [df.loc[df.index[0], y_col]] + [x for x in df[y_col_delta][1:]] + [df.loc[df.index[-1], y_col]]\n\n        return go.Figure(\n            go.Waterfall(\n                *args,\n                measure=measure,\n                x=x,\n                y=y,\n                text=y,\n                **kwargs,\n            ),\n        )\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall.__init__","title":"__init__","text":"<pre><code>__init__(table_name: str | None = None, columns: List[str] | None = None, distinct_columns: List[str] | None = None, dropna: bool = False, delta_prefix: str = '\u0394 ', row_count_column: str = 'Rows')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Specifies the name of the table to log</p> <code>None</code> <code>columns</code> <code>Lists[str]</code> <p>Specifies which columns to log</p> <code>None</code> <code>distinct_columns</code> <code>List[str]</code> <p>Specifies which distinct column values to log</p> <code>None</code> <code>delta_prefix</code> <code>str</code> <p>Prefix for column names with discrete difference (delta) with previous row</p> <code>'\u0394 '</code> <code>dropna</code> <code>bool</code> <p>Whether to exclude NaN in the row counts</p> <code>False</code> <code>row_count_column</code> <code>str</code> <p>Column name for an added column that counts rows in table</p> <code>'Rows'</code> Source code in <code>waterfall_logging/log.py</code> <pre><code>def __init__(\n    self,\n    table_name: str | None = None,\n    columns: List[str] | None = None,\n    distinct_columns: List[str] | None = None,\n    dropna: bool = False,\n    delta_prefix: str = \"\u0394 \",\n    row_count_column: str = \"Rows\",\n):\n\"\"\"\n\n    Args:\n        table_name (str): Specifies the name of the table to log\n        columns (Lists[str]): Specifies which columns to log\n        distinct_columns (List[str]): Specifies which distinct column values to log\n        delta_prefix (str): Prefix for column names with discrete difference (delta) with previous row\n        dropna (bool): Whether to exclude NaN in the row counts\n        row_count_column (str): Column name for an added column that counts rows in table\n\n    \"\"\"\n    self.table_name = table_name\n    self.columns = iterable_to_list(columns)\n    self.distinct_columns = iterable_to_list(distinct_columns)\n    self.delta_prefix = delta_prefix\n    self.dropna = dropna\n    self.row_count_column = row_count_column\n\n    self._input_columns = self.columns + self.distinct_columns\n    self._all_columns = self._input_columns.copy()\n    if self.row_count_column:\n        self._all_columns.append(self.row_count_column)\n    self._static_columns = [\"Table\", \"Reason\", \"Configurations flag\"]\n    self._log = None\n\n    distinct_overwrite = set(self.columns).intersection(set(self.distinct_columns))\n    if distinct_overwrite:\n        warnings.warn(\n            f\"Column names in `distinct_columns` overwrite names in `columns` with distinct counts: \"\n            f\"{distinct_overwrite}.\",\n        )\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall.log","title":"log","text":"<pre><code>log(table: pd.DataFrame | pyspark.sql.DataFrame, reason: str | None = None, configuration_flag: str | None = None, table_name: str | None = None) -&gt; None\n</code></pre> <p>Logs table (distinct) counts to logging DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>pd.DataFrame</code> <p>DataFrame that the filtering is applied to</p> required <code>reason</code> <code>str</code> <p>Specifies reasoning for DataFrame filtering step</p> <code>None</code> <code>configuration_flag</code> <code>str</code> <p>Specifies configurations flag used for DataFrame filtering step</p> <code>None</code> <code>table_name</code> <code>str</code> <p>First column in table is the <code>table id</code> column that should contain the table name value</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; waterfall.log(table, reason='Filtered in-scope bicycles', configuration_flag='inscope=True',\n... table_name='sample_table')\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def log(\n    self,\n    table: pd.DataFrame | pyspark.sql.DataFrame,\n    reason: str | None = None,\n    configuration_flag: str | None = None,\n    table_name: str | None = None,\n) -&gt; None:\n\"\"\"Logs table (distinct) counts to logging DataFrame.\n\n    Args:\n        table (pd.DataFrame): DataFrame that the filtering is applied to\n        reason (str): Specifies reasoning for DataFrame filtering step\n        configuration_flag (str): Specifies configurations flag used for DataFrame filtering step\n        table_name (str): First column in table is the `table id` column that should contain the table name value\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; waterfall.log(table, reason='Filtered in-scope bicycles', configuration_flag='inscope=True',\n        ... table_name='sample_table')\n\n    Returns:\n        None\n\n    \"\"\"\n    table_name = table_name or self.table_name\n\n    if self._log is None:\n        self._log = self._create_log()\n\n    current_table = self._log.loc[self._log.iloc[:, 0] == table_name]\n    entries = self._count_entries(table)\n\n    prior_entries = (\n        entries.copy() if current_table.empty else current_table[self._all_columns].iloc[-1, :].to_list()\n    )\n\n    column_entries = []\n    for entry, prior_entry in zip(entries, prior_entries):\n        column_entries += [entry, entry - prior_entry]\n\n    calculated_columns = [table_name] + column_entries + [reason, configuration_flag]\n\n    self._log.loc[len(self._log)] = calculated_columns\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall.plot","title":"plot","text":"<pre><code>plot(*args, y_col: str = 'Rows', y_col_delta: str = '\u0394 Rows', x_col: str = 'Reason', drop_zero_delta: bool = False, **kwargs) -&gt; go.Figure\n</code></pre> <p>Plots a logging DataFrame column in a waterfall chart.</p> <p>Parameters:</p> Name Type Description Default <code>y_col</code> <code>str</code> <p>Specifies column that contains absolute value counts for y-axis of plot</p> <code>'Rows'</code> <code>y_col_delta</code> <code>str</code> <p>Specifies column that contains delta value counts for y-axis of plot</p> <code>'\u0394 Rows'</code> <code>x_col</code> <code>str</code> <p>Specifies column that contains the filtering explanation for x-axis of plot</p> <code>'Reason'</code> <code>drop_zero_delta</code> <code>bool</code> <p>Whether to remove rows for <code>y_col_delta</code> that contain zeros</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; fig = waterfall.plot(y_col='Rows', y_col_delta='\u0394 Rows', x_col='Reason',\n... textfont=dict(family='sans-serif', size=11),\n... connector={'line': {'color': 'rgba(0,0,0,0)'}},\n... totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n... )\n</code></pre> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: waterfall chart</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def plot(\n    self,\n    *args,\n    y_col: str = \"Rows\",\n    y_col_delta: str = \"\u0394 Rows\",\n    x_col: str = \"Reason\",\n    drop_zero_delta: bool = False,\n    **kwargs,\n) -&gt; go.Figure:\n\"\"\"Plots a logging DataFrame column in a waterfall chart.\n\n    Args:\n        y_col (str): Specifies column that contains absolute value counts for y-axis of plot\n        y_col_delta (str): Specifies column that contains delta value counts for y-axis of plot\n        x_col (str): Specifies column that contains the filtering explanation for x-axis of plot\n        drop_zero_delta (bool): Whether to remove rows for `y_col_delta` that contain zeros\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; fig = waterfall.plot(y_col='Rows', y_col_delta='\u0394 Rows', x_col='Reason',\n        ... textfont=dict(family='sans-serif', size=11),\n        ... connector={'line': {'color': 'rgba(0,0,0,0)'}},\n        ... totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n        ... )\n\n    Returns:\n        go.Figure: waterfall chart\n\n    \"\"\"\n    df = self._log.copy()\n    if drop_zero_delta:\n        df = df.iloc[1:]\n        indices = df[(df[y_col_delta] == 0)].index\n        df = df.drop(indices, inplace=False)\n\n    measure = [\"absolute\"] + [\"relative\"] * (df.shape[0] - 1) + [\"total\"]\n    x = df[x_col].to_list() + [\"Total\"]\n    y = [df.loc[df.index[0], y_col]] + [x for x in df[y_col_delta][1:]] + [df.loc[df.index[-1], y_col]]\n\n    return go.Figure(\n        go.Waterfall(\n            *args,\n            measure=measure,\n            x=x,\n            y=y,\n            text=y,\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall.read_markdown","title":"read_markdown","text":"<pre><code>read_markdown(filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], columns: List[str] | None = None, *args, **kwargs) -&gt; None\n</code></pre> <p>Reads table from Markdown file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath_or_buffer</code> <code>str, path object or file-like object</code> <p>Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any <code>os.PathLike</code>. By file-like object, we refer to objects with a <code>read()</code> method, such as a file handle (e.g. via builtin <code>open</code> function) or <code>StringIO</code>.</p> required <code>columns</code> <code>Lists[str]</code> <p>Specifies which column to read in and log</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; f = open('output/tests/read_markdown_table.md', 'r')\n&gt;&gt;&gt; print(f.read())\n| Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag   |\n|:-------|-------:|---------:|-------:|---------:|:--------|:----------------------|\n| table1 |     50 |        0 |   2727 |        0 | initial | read_markdown         |\n| table1 |    150 |      100 |   2827 |        0 | add-on  | read_markdown         |\n| table1 |    250 |      100 |   2927 |      100 | extra   | read_markdown         |\n&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; waterfall.read_markdown(\n...     filepath_or_buffer='output/tests/read_markdown_table.md',\n....    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n... )\n&gt;&gt;&gt; print(waterfall._log)\n    Table  col1  \u0394 col1  Rows  \u0394 Rows   Reason Configurations flag\n0  table1    50       0  2727       0  example       read_markdown\n1  table1   150     100  2827       0  add-on        read_markdown\n1  table1   250     100  2927       0  extra         read_markdown\n&gt;&gt;&gt; print(type(waterfall._log))\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def read_markdown(\n    self,\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    columns: List[str] | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n\"\"\"Reads table from Markdown file.\n\n    Args:\n        filepath_or_buffer (str, path object or file-like object):\n            Any valid string path is acceptable. The string could be a URL. Valid\n            URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n            expected. A local file could be: file://localhost/path/to/table.csv.\n            If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n            By file-like object, we refer to objects with a ``read()`` method, such as\n            a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n        columns (Lists[str]): Specifies which column to read in and log\n\n    Examples:\n        &gt;&gt;&gt; f = open('output/tests/read_markdown_table.md', 'r')\n        &gt;&gt;&gt; print(f.read())\n        | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag   |\n        |:-------|-------:|---------:|-------:|---------:|:--------|:----------------------|\n        | table1 |     50 |        0 |   2727 |        0 | initial | read_markdown         |\n        | table1 |    150 |      100 |   2827 |        0 | add-on  | read_markdown         |\n        | table1 |    250 |      100 |   2927 |      100 | extra   | read_markdown         |\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; waterfall.read_markdown(\n        ...     filepath_or_buffer='output/tests/read_markdown_table.md',\n        ....    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n        ... )\n        &gt;&gt;&gt; print(waterfall._log)\n            Table  col1  \u0394 col1  Rows  \u0394 Rows   Reason Configurations flag\n        0  table1    50       0  2727       0  example       read_markdown\n        1  table1   150     100  2827       0  add-on        read_markdown\n        1  table1   250     100  2927       0  extra         read_markdown\n        &gt;&gt;&gt; print(type(waterfall._log))\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n    Returns:\n        None\n\n    \"\"\"\n    self._log = (\n        pd.read_table(filepath_or_buffer, *args, **kwargs)\n        # strips trailing whitespaces in column names\n        # drops columns with all NA values\n        .rename(str.rstrip, axis=\"columns\").dropna(axis=1, how=\"all\")\n        # strips trailing whitespaces in column values\n        .apply(lambda row: row.str.rstrip() if row.dtype == object else row)\n    )\n    self._log = self._log[columns] if columns else self._log\n</code></pre>"},{"location":"explanations/log/#waterfall_logging.log.Waterfall.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown(*args, index = False, **kwargs)\n</code></pre> <p>Print DataFrame in Markdown-friendly format.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>bool</code> <p>Add index (row) labels</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; print(waterfall.to_markdown(index=True))\n|    | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n|---:|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n|  0 | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n|  1 | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n&gt;&gt;&gt; print(waterfall.to_markdown(index=False))\n| Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n| table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n| table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>DataFrame in Markdown-friendly format</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def to_markdown(self, *args, index=False, **kwargs):\n\"\"\"Print DataFrame in Markdown-friendly format.\n\n    Args:\n        index (bool): Add index (row) labels\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; print(waterfall.to_markdown(index=True))\n        |    | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n        |---:|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n        |  0 | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n        |  1 | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n        &gt;&gt;&gt; print(waterfall.to_markdown(index=False))\n        | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n        |:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n        | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n        | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n\n    Returns:\n        (str): DataFrame in Markdown-friendly format\n\n    \"\"\"\n    return self._log.to_markdown(*args, index=index, **kwargs)\n</code></pre>"},{"location":"how-to-guides/base_class/","title":"Base class","text":"<p>This part of the project documentation focuses on a task-oriented approach. Use it as a guide to accomplish implementing your own <code>Waterfall</code> class.</p>"},{"location":"how-to-guides/base_class/#abstract-methods","title":"Abstract methods","text":"<p>The  <code>PandasWaterfall</code> and <code>SparkWaterfall</code> classes inherent from the <code>Waterfall</code> base class. The base <code>Waterfall</code> class has two abstract methods that need to implemented in any class inheriting from it.</p>"},{"location":"how-to-guides/base_class/#_count_or_distinct","title":"_count_or_distinct","text":"<p>Column count for <code>self.columns</code> and distinct column count for <code>self.distinct_columns</code> including NaNs.</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>@abc.abstractmethod\ndef _count_or_distinct(self, table) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` including NaNs.\"\"\"\n</code></pre>"},{"location":"how-to-guides/base_class/#_count_or_distinct_dropna","title":"_count_or_distinct_dropna","text":"<p>Column count for <code>self.columns</code> and distinct column count for <code>self.distinct_columns</code> excluding NaNs.</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>@abc.abstractmethod\ndef _count_or_distinct_dropna(self, table) -&gt; List[int]:\n\"\"\"Column count for `self.columns` and distinct column count for `self.distinct_columns` excluding NaNs.\"\"\"\n</code></pre>"},{"location":"how-to-guides/base_class/#contributions","title":"Contributions","text":"<p>If you find a better way to implement these functions (for instance, a computationally less expensive way in Spark), please contribute to this package in Github!</p>"},{"location":"how-to-guides/waterfall/","title":"Waterfall","text":"<p>This part of the project documentation focuses on a task-oriented approach. Use it as a guide to accomplish any of the <code>Waterfall</code> class methods below.</p>"},{"location":"how-to-guides/waterfall/#count","title":"Count","text":"<p>Count columns via the <code>columns</code> parameter in the <code>__init__</code> of the <code>PandasWaterfall</code> or <code>SparkWaterfall</code> class. An example is provided below.</p> <pre><code>from waterfall_logging.log import PandasWaterfall, SparkWaterfall\n\npandas_w = PandasWaterfall(columns=['user_id'])\n\nspark_w = SparkWaterfall(columns=['user_id'])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Specifies the name of the table to log</p> <code>None</code> <code>columns</code> <code>Lists[str]</code> <p>Specifies which columns to log</p> <code>None</code> <code>distinct_columns</code> <code>List[str]</code> <p>Specifies which distinct column values to log</p> <code>None</code> <code>delta_prefix</code> <code>str</code> <p>Prefix for column names with discrete difference (delta) with previous row</p> <code>'\u0394 '</code> <code>dropna</code> <code>bool</code> <p>Whether to exclude NaN in the row counts</p> <code>False</code> <code>row_count_column</code> <code>str</code> <p>Column name for an added column that counts rows in table</p> <code>'Rows'</code> Source code in <code>waterfall_logging/log.py</code> <pre><code>def __init__(\n    self,\n    table_name: str | None = None,\n    columns: List[str] | None = None,\n    distinct_columns: List[str] | None = None,\n    dropna: bool = False,\n    delta_prefix: str = \"\u0394 \",\n    row_count_column: str = \"Rows\",\n):\n\"\"\"\n\n    Args:\n        table_name (str): Specifies the name of the table to log\n        columns (Lists[str]): Specifies which columns to log\n        distinct_columns (List[str]): Specifies which distinct column values to log\n        delta_prefix (str): Prefix for column names with discrete difference (delta) with previous row\n        dropna (bool): Whether to exclude NaN in the row counts\n        row_count_column (str): Column name for an added column that counts rows in table\n\n    \"\"\"\n    self.table_name = table_name\n    self.columns = iterable_to_list(columns)\n    self.distinct_columns = iterable_to_list(distinct_columns)\n    self.delta_prefix = delta_prefix\n    self.dropna = dropna\n    self.row_count_column = row_count_column\n\n    self._input_columns = self.columns + self.distinct_columns\n    self._all_columns = self._input_columns.copy()\n    if self.row_count_column:\n        self._all_columns.append(self.row_count_column)\n    self._static_columns = [\"Table\", \"Reason\", \"Configurations flag\"]\n    self._log = None\n\n    distinct_overwrite = set(self.columns).intersection(set(self.distinct_columns))\n    if distinct_overwrite:\n        warnings.warn(\n            f\"Column names in `distinct_columns` overwrite names in `columns` with distinct counts: \"\n            f\"{distinct_overwrite}.\",\n        )\n</code></pre>"},{"location":"how-to-guides/waterfall/#count-distinct","title":"Count distinct","text":"<p>Count distinct columns via the <code>distinct_columns</code> parameter in the <code>__init__</code> of the <code>PandasWaterfall</code> or <code>SparkWaterfall</code> class. An example is provided below.</p> <pre><code>from waterfall_logging.log import PandasWaterfall, SparkWaterfall\n\npandas_w = PandasWaterfall(distinct_columns=['user_id'])\n\nspark_w = SparkWaterfall(distinct_columns=['user_id'])\n</code></pre> <p>Note</p> <p>Column names in <code>distinct_columns</code> overwrite names in <code>columns</code> with distinct counts.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Specifies the name of the table to log</p> <code>None</code> <code>columns</code> <code>Lists[str]</code> <p>Specifies which columns to log</p> <code>None</code> <code>distinct_columns</code> <code>List[str]</code> <p>Specifies which distinct column values to log</p> <code>None</code> <code>delta_prefix</code> <code>str</code> <p>Prefix for column names with discrete difference (delta) with previous row</p> <code>'\u0394 '</code> <code>dropna</code> <code>bool</code> <p>Whether to exclude NaN in the row counts</p> <code>False</code> <code>row_count_column</code> <code>str</code> <p>Column name for an added column that counts rows in table</p> <code>'Rows'</code> Source code in <code>waterfall_logging/log.py</code> <pre><code>def __init__(\n    self,\n    table_name: str | None = None,\n    columns: List[str] | None = None,\n    distinct_columns: List[str] | None = None,\n    dropna: bool = False,\n    delta_prefix: str = \"\u0394 \",\n    row_count_column: str = \"Rows\",\n):\n\"\"\"\n\n    Args:\n        table_name (str): Specifies the name of the table to log\n        columns (Lists[str]): Specifies which columns to log\n        distinct_columns (List[str]): Specifies which distinct column values to log\n        delta_prefix (str): Prefix for column names with discrete difference (delta) with previous row\n        dropna (bool): Whether to exclude NaN in the row counts\n        row_count_column (str): Column name for an added column that counts rows in table\n\n    \"\"\"\n    self.table_name = table_name\n    self.columns = iterable_to_list(columns)\n    self.distinct_columns = iterable_to_list(distinct_columns)\n    self.delta_prefix = delta_prefix\n    self.dropna = dropna\n    self.row_count_column = row_count_column\n\n    self._input_columns = self.columns + self.distinct_columns\n    self._all_columns = self._input_columns.copy()\n    if self.row_count_column:\n        self._all_columns.append(self.row_count_column)\n    self._static_columns = [\"Table\", \"Reason\", \"Configurations flag\"]\n    self._log = None\n\n    distinct_overwrite = set(self.columns).intersection(set(self.distinct_columns))\n    if distinct_overwrite:\n        warnings.warn(\n            f\"Column names in `distinct_columns` overwrite names in `columns` with distinct counts: \"\n            f\"{distinct_overwrite}.\",\n        )\n</code></pre>"},{"location":"how-to-guides/waterfall/#drop-nan","title":"Drop NaN","text":"<p>Drop NaN values in the (distinct) counts of your Pandas or Spark DataFrame.</p> <pre><code>from waterfall_logging.log import PandasWaterfall, SparkWaterfall\n\npandas_w = PandasWaterfall(columns=['A'], dropna=True)\n\nspark_w = SparkWaterfall(columns=['A'], dropna=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Specifies the name of the table to log</p> <code>None</code> <code>columns</code> <code>Lists[str]</code> <p>Specifies which columns to log</p> <code>None</code> <code>distinct_columns</code> <code>List[str]</code> <p>Specifies which distinct column values to log</p> <code>None</code> <code>delta_prefix</code> <code>str</code> <p>Prefix for column names with discrete difference (delta) with previous row</p> <code>'\u0394 '</code> <code>dropna</code> <code>bool</code> <p>Whether to exclude NaN in the row counts</p> <code>False</code> <code>row_count_column</code> <code>str</code> <p>Column name for an added column that counts rows in table</p> <code>'Rows'</code> Source code in <code>waterfall_logging/log.py</code> <pre><code>def __init__(\n    self,\n    table_name: str | None = None,\n    columns: List[str] | None = None,\n    distinct_columns: List[str] | None = None,\n    dropna: bool = False,\n    delta_prefix: str = \"\u0394 \",\n    row_count_column: str = \"Rows\",\n):\n\"\"\"\n\n    Args:\n        table_name (str): Specifies the name of the table to log\n        columns (Lists[str]): Specifies which columns to log\n        distinct_columns (List[str]): Specifies which distinct column values to log\n        delta_prefix (str): Prefix for column names with discrete difference (delta) with previous row\n        dropna (bool): Whether to exclude NaN in the row counts\n        row_count_column (str): Column name for an added column that counts rows in table\n\n    \"\"\"\n    self.table_name = table_name\n    self.columns = iterable_to_list(columns)\n    self.distinct_columns = iterable_to_list(distinct_columns)\n    self.delta_prefix = delta_prefix\n    self.dropna = dropna\n    self.row_count_column = row_count_column\n\n    self._input_columns = self.columns + self.distinct_columns\n    self._all_columns = self._input_columns.copy()\n    if self.row_count_column:\n        self._all_columns.append(self.row_count_column)\n    self._static_columns = [\"Table\", \"Reason\", \"Configurations flag\"]\n    self._log = None\n\n    distinct_overwrite = set(self.columns).intersection(set(self.distinct_columns))\n    if distinct_overwrite:\n        warnings.warn(\n            f\"Column names in `distinct_columns` overwrite names in `columns` with distinct counts: \"\n            f\"{distinct_overwrite}.\",\n        )\n</code></pre>"},{"location":"how-to-guides/waterfall/#log-waterfall-step","title":"Log Waterfall step","text":"<p>Logs a Pandas or Spark DataFrame.</p> <pre><code>import pandas as pd\nimport pyspark\n\nfrom waterfall_logging.log import PandasWaterfall, SparkWaterfall\n\ndf = pd.DataFrame(data={'A': [0,1,2,3,4,5], 'B': ['id1', 'id2', 'id3', 'id4', 'id4']})\n\npandas_w = PandasWaterfall(columns=['A'], distinct_columns=['B'])\npandas_w.log(table=df, reason='example', configuration_flag='initial')\n\nspark = pyspark.sql.SparkSession.builder.enableHiveSupport().appName(str(__file__)).getOrCreate()\nsdf = spark.createDataFrame(df)\n\nspark_w = SparkWaterfall(columns=['A'], distinct_columns=['B'])\nspark_w.log(table=sdf, reason='example', configuration_flag='initial')\n</code></pre> <p>Logs table (distinct) counts to logging DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>pd.DataFrame</code> <p>DataFrame that the filtering is applied to</p> required <code>reason</code> <code>str</code> <p>Specifies reasoning for DataFrame filtering step</p> <code>None</code> <code>configuration_flag</code> <code>str</code> <p>Specifies configurations flag used for DataFrame filtering step</p> <code>None</code> <code>table_name</code> <code>str</code> <p>First column in table is the <code>table id</code> column that should contain the table name value</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; waterfall.log(table, reason='Filtered in-scope bicycles', configuration_flag='inscope=True',\n... table_name='sample_table')\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def log(\n    self,\n    table: pd.DataFrame | pyspark.sql.DataFrame,\n    reason: str | None = None,\n    configuration_flag: str | None = None,\n    table_name: str | None = None,\n) -&gt; None:\n\"\"\"Logs table (distinct) counts to logging DataFrame.\n\n    Args:\n        table (pd.DataFrame): DataFrame that the filtering is applied to\n        reason (str): Specifies reasoning for DataFrame filtering step\n        configuration_flag (str): Specifies configurations flag used for DataFrame filtering step\n        table_name (str): First column in table is the `table id` column that should contain the table name value\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; waterfall.log(table, reason='Filtered in-scope bicycles', configuration_flag='inscope=True',\n        ... table_name='sample_table')\n\n    Returns:\n        None\n\n    \"\"\"\n    table_name = table_name or self.table_name\n\n    if self._log is None:\n        self._log = self._create_log()\n\n    current_table = self._log.loc[self._log.iloc[:, 0] == table_name]\n    entries = self._count_entries(table)\n\n    prior_entries = (\n        entries.copy() if current_table.empty else current_table[self._all_columns].iloc[-1, :].to_list()\n    )\n\n    column_entries = []\n    for entry, prior_entry in zip(entries, prior_entries):\n        column_entries += [entry, entry - prior_entry]\n\n    calculated_columns = [table_name] + column_entries + [reason, configuration_flag]\n\n    self._log.loc[len(self._log)] = calculated_columns\n</code></pre>"},{"location":"how-to-guides/waterfall/#write-to-markdown-file","title":"Write to Markdown file","text":"<p>Write Waterfall logging tables to Markdown files.</p> <p>Under the hood uses pandas.to_markdown. All arguments that can be provided for <code>pandas.to_markdown()</code> function, can also be used in this function.</p> <p>Print DataFrame in Markdown-friendly format.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>bool</code> <p>Add index (row) labels</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; print(waterfall.to_markdown(index=True))\n|    | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n|---:|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n|  0 | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n|  1 | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n&gt;&gt;&gt; print(waterfall.to_markdown(index=False))\n| Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n| table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n| table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>DataFrame in Markdown-friendly format</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def to_markdown(self, *args, index=False, **kwargs):\n\"\"\"Print DataFrame in Markdown-friendly format.\n\n    Args:\n        index (bool): Add index (row) labels\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; print(waterfall.to_markdown(index=True))\n        |    | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n        |---:|:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n        |  0 | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n        |  1 | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n        &gt;&gt;&gt; print(waterfall.to_markdown(index=False))\n        | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag |\n        |:-------|-------:|---------:|-------:|---------:|:--------|:--------------------|\n        | table1 |     50 |        0 |   2727 |        0 | example | to_markdown         |\n        | table1 |    150 |      100 |   2827 |        0 | example | to_markdown         |\n\n    Returns:\n        (str): DataFrame in Markdown-friendly format\n\n    \"\"\"\n    return self._log.to_markdown(*args, index=index, **kwargs)\n</code></pre>"},{"location":"how-to-guides/waterfall/#read-from-markdown-file","title":"Read from Markdown file","text":"<p>Read Waterfall logging tables from Markdown files.</p> <p>Under the hood uses pandas.read_table. All arguments that can be provided for <code>pandas.read_table()</code>, can also be used in this function.</p> <p>Some useful arguments:</p> <ul> <li>The seperator in our markdown file is a pipe: <code>sep=|</code>.</li> <li>The header with column names start at row index: <code>header=0</code>.</li> <li>The markdown file does not contain an index column: <code>index_col=False</code>.</li> <li>The row that separates the headers from the values is on the first row: <code>skiprows=[1]</code>.</li> <li>Markdowns column names are preceded with initial spaces, to be removed when reading: <code>skipinitialspace=True</code>.</li> </ul> <p>Reads table from Markdown file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath_or_buffer</code> <code>str, path object or file-like object</code> <p>Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any <code>os.PathLike</code>. By file-like object, we refer to objects with a <code>read()</code> method, such as a file handle (e.g. via builtin <code>open</code> function) or <code>StringIO</code>.</p> required <code>columns</code> <code>Lists[str]</code> <p>Specifies which column to read in and log</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; f = open('output/tests/read_markdown_table.md', 'r')\n&gt;&gt;&gt; print(f.read())\n| Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag   |\n|:-------|-------:|---------:|-------:|---------:|:--------|:----------------------|\n| table1 |     50 |        0 |   2727 |        0 | initial | read_markdown         |\n| table1 |    150 |      100 |   2827 |        0 | add-on  | read_markdown         |\n| table1 |    250 |      100 |   2927 |      100 | extra   | read_markdown         |\n&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; waterfall.read_markdown(\n...     filepath_or_buffer='output/tests/read_markdown_table.md',\n....    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n... )\n&gt;&gt;&gt; print(waterfall._log)\n    Table  col1  \u0394 col1  Rows  \u0394 Rows   Reason Configurations flag\n0  table1    50       0  2727       0  example       read_markdown\n1  table1   150     100  2827       0  add-on        read_markdown\n1  table1   250     100  2927       0  extra         read_markdown\n&gt;&gt;&gt; print(type(waterfall._log))\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def read_markdown(\n    self,\n    filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n    columns: List[str] | None = None,\n    *args,\n    **kwargs,\n) -&gt; None:\n\"\"\"Reads table from Markdown file.\n\n    Args:\n        filepath_or_buffer (str, path object or file-like object):\n            Any valid string path is acceptable. The string could be a URL. Valid\n            URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n            expected. A local file could be: file://localhost/path/to/table.csv.\n            If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n            By file-like object, we refer to objects with a ``read()`` method, such as\n            a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n        columns (Lists[str]): Specifies which column to read in and log\n\n    Examples:\n        &gt;&gt;&gt; f = open('output/tests/read_markdown_table.md', 'r')\n        &gt;&gt;&gt; print(f.read())\n        | Table  |   col1 |   \u0394 col1 |   Rows |   \u0394 Rows | Reason  | Configurations flag   |\n        |:-------|-------:|---------:|-------:|---------:|:--------|:----------------------|\n        | table1 |     50 |        0 |   2727 |        0 | initial | read_markdown         |\n        | table1 |    150 |      100 |   2827 |        0 | add-on  | read_markdown         |\n        | table1 |    250 |      100 |   2927 |      100 | extra   | read_markdown         |\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; waterfall.read_markdown(\n        ...     filepath_or_buffer='output/tests/read_markdown_table.md',\n        ....    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n        ... )\n        &gt;&gt;&gt; print(waterfall._log)\n            Table  col1  \u0394 col1  Rows  \u0394 Rows   Reason Configurations flag\n        0  table1    50       0  2727       0  example       read_markdown\n        1  table1   150     100  2827       0  add-on        read_markdown\n        1  table1   250     100  2927       0  extra         read_markdown\n        &gt;&gt;&gt; print(type(waterfall._log))\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n    Returns:\n        None\n\n    \"\"\"\n    self._log = (\n        pd.read_table(filepath_or_buffer, *args, **kwargs)\n        # strips trailing whitespaces in column names\n        # drops columns with all NA values\n        .rename(str.rstrip, axis=\"columns\").dropna(axis=1, how=\"all\")\n        # strips trailing whitespaces in column values\n        .apply(lambda row: row.str.rstrip() if row.dtype == object else row)\n    )\n    self._log = self._log[columns] if columns else self._log\n</code></pre>"},{"location":"how-to-guides/waterfall/#plot-waterfall","title":"Plot Waterfall","text":"<p>Plots Waterfall logging tables in a chart.</p> <p>Under the hood plots uses a Plotly Waterfall chart plotly.graph_objects.Waterfall.</p> <p>Note</p> <p>Make sure to use an unique <code>reason</code> arguments for each logging step, otherwise your plot will look strange!</p> <p>Plots a logging DataFrame column in a waterfall chart.</p> <p>Parameters:</p> Name Type Description Default <code>y_col</code> <code>str</code> <p>Specifies column that contains absolute value counts for y-axis of plot</p> <code>'Rows'</code> <code>y_col_delta</code> <code>str</code> <p>Specifies column that contains delta value counts for y-axis of plot</p> <code>'\u0394 Rows'</code> <code>x_col</code> <code>str</code> <p>Specifies column that contains the filtering explanation for x-axis of plot</p> <code>'Reason'</code> <code>drop_zero_delta</code> <code>bool</code> <p>Whether to remove rows for <code>y_col_delta</code> that contain zeros</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; waterfall = PandasWaterfall()\n&gt;&gt;&gt; fig = waterfall.plot(y_col='Rows', y_col_delta='\u0394 Rows', x_col='Reason',\n... textfont=dict(family='sans-serif', size=11),\n... connector={'line': {'color': 'rgba(0,0,0,0)'}},\n... totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n... )\n</code></pre> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: waterfall chart</p> Source code in <code>waterfall_logging/log.py</code> <pre><code>def plot(\n    self,\n    *args,\n    y_col: str = \"Rows\",\n    y_col_delta: str = \"\u0394 Rows\",\n    x_col: str = \"Reason\",\n    drop_zero_delta: bool = False,\n    **kwargs,\n) -&gt; go.Figure:\n\"\"\"Plots a logging DataFrame column in a waterfall chart.\n\n    Args:\n        y_col (str): Specifies column that contains absolute value counts for y-axis of plot\n        y_col_delta (str): Specifies column that contains delta value counts for y-axis of plot\n        x_col (str): Specifies column that contains the filtering explanation for x-axis of plot\n        drop_zero_delta (bool): Whether to remove rows for `y_col_delta` that contain zeros\n\n    Examples:\n        &gt;&gt;&gt; waterfall = PandasWaterfall()\n        &gt;&gt;&gt; fig = waterfall.plot(y_col='Rows', y_col_delta='\u0394 Rows', x_col='Reason',\n        ... textfont=dict(family='sans-serif', size=11),\n        ... connector={'line': {'color': 'rgba(0,0,0,0)'}},\n        ... totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n        ... )\n\n    Returns:\n        go.Figure: waterfall chart\n\n    \"\"\"\n    df = self._log.copy()\n    if drop_zero_delta:\n        df = df.iloc[1:]\n        indices = df[(df[y_col_delta] == 0)].index\n        df = df.drop(indices, inplace=False)\n\n    measure = [\"absolute\"] + [\"relative\"] * (df.shape[0] - 1) + [\"total\"]\n    x = df[x_col].to_list() + [\"Total\"]\n    y = [df.loc[df.index[0], y_col]] + [x for x in df[y_col_delta][1:]] + [df.loc[df.index[-1], y_col]]\n\n    return go.Figure(\n        go.Waterfall(\n            *args,\n            measure=measure,\n            x=x,\n            y=y,\n            text=y,\n            **kwargs,\n        ),\n    )\n</code></pre> <p>See below for a more in-depth example, including <code>fig.update_layout</code> and <code>fig.update_traces</code>.</p> <p><pre><code>from waterfall_logging.log import PandasWaterfall\n\nwaterfall_log = PandasWaterfall()\nwaterfall_log.read_markdown(filepath_or_buffer='output/tests/read_markdown_table.md',\n    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n)\n\nfig = waterfall_log.plot(y_col='Rows',\n    textfont=dict(family='sans-serif', size=11),\n    connector={'line': {'color': 'rgba(0,0,0,0)'}},\n    totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n)\n\nfig.update_layout(\n    autosize=True,\n    width=1000,\n    height=1000,\n    title=f'Data filtering steps',\n    xaxis=dict(title='Filtering steps'),\n    yaxis=dict(title='# of entries'),\n    showlegend=False,\n    waterfallgroupgap=0.1,\n)\n\nfig.update_traces(\n    textposition='outside',\n)\n\nfig.write_image('output/tests/read_markdown_table.png')\n</code></pre> </p>"},{"location":"tutorials/context_manager/","title":"Context manager","text":"<p>This part of the project documentation focuses on a learning-oriented approach. Use the tutorial below to learn how the <code>waterfall</code> context manager as a function decorator can be useful.</p> <pre><code>import pandas as pd\nfrom waterfall_logging.log import PandasWaterfall\nfrom waterfall_logging.context_manager import waterfall\n\n\nwaterfall_log = PandasWaterfall(table_name=\"context_manager_markdown\", columns=[\"size\"])\n\n\ndef filter_brands(bicycle_rides: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\"\"\"\n    return bicycle_rides[bicycle_rides[\"brand\"].isin([\"Gazelle\", \"Batavia\"])]\n\n\ndef filter_small_wheels(bicycle_rides: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\"\"\"\n    return bicycle_rides[bicycle_rides[\"wheel_size\"] &gt;= 31]\n\n\n@waterfall(log=waterfall_log, variable_names=[\"bicycle_rides\"], markdown_kwargs={'buf': 'path_to_save_markdown.md'})\ndef main():\n\"\"\"\"\"\"\n    bicycle_rides = pd.DataFrame(data=[\n        ['Shimano', 'race', 28, '2023-02-13', 1],\n        ['Gazelle', 'comfort', 31, '2023-02-15', 1],\n        ['Shimano', 'race', 31, '2023-02-16', 2],\n        ['Batavia', 'comfort', 30, '2023-02-17', 3],\n    ], columns=['brand', 'ride_type', 'wheel_size', 'date', 'bike_id'])\n\n    bicycle_rides = filter_brands(bicycle_rides)\n\n    bicycle_rides = filter_small_wheels(bicycle_rides)\n\n    print(waterfall_log.to_markdown())\n\"\"\"\n    | Table                    |   size |   \u0394 size |   Rows |   \u0394 Rows | Reason   | Configurations flag   |\n    |:-------------------------|-------:|---------:|-------:|---------:|:---------|:----------------------|\n    | context_manager_markdown |      4 |        0 |      4 |        0 |          |                       |\n    | context_manager_markdown |      2 |       -2 |      2 |       -2 |          |                       |\n    | context_manager_markdown |      1 |       -1 |      1 |       -1 |          |                       |\n    \"\"\"\n\n    print(\"The `waterfall_log` is also saved in the file `path_to_save_markdown.md`!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/decorator/","title":"Decorator","text":"<p>This part of the project documentation focuses on a learning-oriented approach. Use the tutorial below to learn how the <code>waterfall</code> function decorator can be useful.</p> <pre><code>import pandas as pd\nfrom waterfall_logging.log import PandasWaterfall\nfrom waterfall_logging.decorator import waterfall\n\n\nwaterfall_log = PandasWaterfall(table_name=\"decorator_markdown\", columns=]\"size\"])\n\n\n@waterfall(log=waterfall_log, reason=\"filter brands\", configuration_flag=f'{[\"Gazelle\", \"Batavia\"]}')\ndef filter_brands(bicycle_rides: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\"\"\"\n    return bicycle_rides[bicycle_rides[\"brand\"].isin([\"Gazelle\", \"Batavia\"])]\n\n\n@waterfall(log=waterfall_log, reason=\"filter small wheels\", configuration_flag=\"&gt;= 31\")\ndef filter_small_wheels(bicycle_rides: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\"\"\"\n    return bicycle_rides[bicycle_rides[\"wheel_size\"] &gt;= 31]\n\n\ndef main():\n\"\"\"\"\"\"\n    bicycle_rides = pd.DataFrame(data=[\n        ['Shimano', 'race', 28, '2023-02-13', 1],\n        ['Gazelle', 'comfort', 31, '2023-02-15', 1],\n        ['Shimano', 'race', 31, '2023-02-16', 2],\n        ['Batavia', 'comfort', 30, '2023-02-17', 3],\n    ], columns=['brand', 'ride_type', 'wheel_size', 'date', 'bike_id'])\n\n    bicycle_rides = filter_brands(bicycle_rides)\n\n    bicycle_rides = filter_small_wheels(bicycle_rides)\n\n    print(waterfall_log.to_markdown())\n'''\n    | Table              |   size |   \u0394 size |   Rows |   \u0394 Rows | Reason              | Configurations flag    |\n    |:-------------------|-------:|---------:|-------:|---------:|:--------------------|:-----------------------|\n    | decorator_markdown |      2 |        0 |      2 |        0 | filter brands       | ['Gazelle', 'Batavia'] |\n    | decorator_markdown |      1 |       -1 |      1 |       -1 | filter small wheels | &gt;= 31                  |\n    '''\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/pandas/","title":"PandasWaterfall","text":"<p>This part of the project documentation focuses on a learning-oriented approach. Use the tutorial below to learn how this package can be useful.</p>"},{"location":"tutorials/pandas/#california-housing-data","title":"California housing data","text":"<p>In this tutorial you'll learn how the <code>PandasWaterfall</code> can log and plot several filtering steps on the California housing dataset.</p>"},{"location":"tutorials/pandas/#helper-functions","title":"Helper functions","text":"<p>Below you can find several helper functions to scope or filter the dataset.</p> <pre><code>from typing import Dict, Any\n\nimport pandas as pd\n\ndef filter_bedrooms(df: pd.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses with at least one bedroom in the block.\"\"\"\n    return df.loc[lambda row: row['AveBedrms'] &gt;= settings['min_bedrooms']]\n\ndef filter_household_members(df: pd.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses for one or two household members.\"\"\"\n    return df.loc[lambda row: row['AveOccup'] &lt;= settings['max_occupants']]\n\ndef filter_house_age_range(df: pd.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses aged between two ages.\"\"\"\n    return df.loc[\n        lambda row: (row['HouseAge'] &gt;= settings['house_age'][0]) &amp; (row['HouseAge'] &lt;= settings['house_age'][1])\n    ]\n\ndef join_new_house_data(df1: pd.DataFrame, df2: pd.DataFrame):\n\"\"\"Appends houses with an average number of bedrooms in the block between 0.5 and 1.0.\"\"\"\n    return pd.concat(\n        [df1, df2.loc[lambda row: (row['AveBedrms'] &gt;= 0.5) &amp; (row['AveBedrms'] &lt;= 1.0)]],\n        join='outer',\n        ignore_index=True,\n    )\n</code></pre>"},{"location":"tutorials/pandas/#logging","title":"Logging","text":"<p>Below you can find the <code>PandasWaterfall</code> implementation for scoping a dataset imported from scikit-learn.</p> <pre><code>from sklearn import datasets\n\nfrom waterfall_logging.log import PandasWaterfall\n\n\ncalifornia_housing = datasets.fetch_california_housing(as_frame=True)\ncalifornia_housing_df = california_housing.frame\n\nsettings = {\n    'min_bedrooms': 1.0,\n    'max_occupants': 2.0,\n    'house_age': [10, 80],\n    'join_new_houses': True,\n}\n\nwaterfall_log = PandasWaterfall(table_name='california housing', columns=['MedInc', 'AveRooms'],  distinct_columns=['Population'])\nwaterfall_log.log(california_housing_df, reason='Raw california housing data', configuration_flag=None)\n\ncalifornia_housing_df = filter_bedrooms(california_housing_df, settings)\nwaterfall_log.log(california_housing_df, reason='Select in-scope bedrooms', configuration_flag=f\"{settings['min_bedrooms']}\")\n\nif settings['join_new_houses']:\n    waterfall_log.log(california_housing.frame, table_name='virginia housing', reason=\"Load Virginia's housing data\", configuration_flag='n/a')\n\n    california_housing_df = join_new_house_data(california_housing_df, california_housing.frame)\n    waterfall_log.log(california_housing_df, reason='Join houses w/ 0.5 to 1.0 average bedrooms', configuration_flag=f\"{settings['join_new_houses']}\")\n\ncalifornia_housing_df = filter_household_members(california_housing_df, settings)\nwaterfall_log.log(california_housing_df, reason='Maximum two occupants', configuration_flag=f\"{settings['max_occupants']}\")\n\ncalifornia_housing_df = filter_house_age_range(california_housing_df, settings)\nwaterfall_log.log(california_housing_df, reason='House age range', configuration_flag=f\"{settings['house_age']}\")\n\nwaterfall_log.to_markdown('output/tests/california_housing.md')\n\nwaterfall_log = PandasWaterfall(table_name='california housing')\nwaterfall_log.read_markdown('output/tests/california_housing.md',\n    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n)\n</code></pre>"},{"location":"tutorials/pandas/#plotting","title":"Plotting","text":"<p>Below you can see how to plot the markdown table.</p> <pre><code>fig = waterfall_log.plot(y_col='MedInc', y_col_delta='\u0394 MedInc', x_col='Reason', drop_zero_delta=False,\n    textfont=dict(family='sans-serif', size=11),\n    connector={'line': {'color': 'rgba(0,0,0,0)'}},\n    totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n)\n\nfig.update_layout(\n    autosize=True,\n    width=1000,\n    height=1000,\n    title=f'Data filtering steps',\n    xaxis=dict(title='Filtering steps'),\n    yaxis=dict(title='# of entries'),\n    showlegend=False,\n    waterfallgroupgap=0.1,\n)\n\nfig.update_traces(\n    textposition='outside',\n)\n\nfig.write_image('output/tests/california_housing.png')\n</code></pre> <p></p>"},{"location":"tutorials/pyspark/","title":"SparkWaterfall","text":"<p>This part of the project documentation focuses on a learning-oriented approach. Use the tutorial below to learn how this package can be useful.</p>"},{"location":"tutorials/pyspark/#california-housing-data","title":"California housing data","text":"<p>In this tutorial you'll learn how the <code>SparkWaterfall</code> can log and plot several filtering steps on the California housing dataset.</p>"},{"location":"tutorials/pyspark/#helper-functions","title":"Helper functions","text":"<p>Below you can find several helper functions to scope or filter the dataset.</p> <pre><code>from typing import Dict, Any\n\nimport pyspark\nimport pyspark.sql.functions as F\n\ndef filter_bedrooms(sdf: pyspark.sql.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses with at least one bedroom in the block.\"\"\"\n    return sdf.filter(F.col('AveBedrms') &gt;= settings['min_bedrooms'])\n\ndef filter_household_members(sdf: pyspark.sql.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses for one or two household members.\"\"\"\n    return sdf.filter(F.col('AveOccup') &lt;= settings['max_occupants'])\n\ndef filter_house_age_range(sdf: pyspark.sql.DataFrame, settings: Dict[str, Any]):\n\"\"\"Filters houses aged between two ages.\"\"\"\n    return sdf.filter((F.col('HouseAge') &gt;= settings['house_age'][0]) &amp; (F.col('HouseAge') &lt;= settings['house_age'][1]))\n\ndef join_new_house_data(sdf1: pyspark.sql.DataFrame, sdf2: pyspark.sql.DataFrame):\n\"\"\"Appends houses with an average number of bedrooms in the block between 0.5 and 1.0.\"\"\"\n    return sdf1.union(sdf2.filter((F.col('AveBedrms') &gt;= 0.5) &amp; (F.col('AveBedrms') &lt;= 1.0)))\n</code></pre>"},{"location":"tutorials/pyspark/#logging","title":"Logging","text":"<p>Below you can find the <code>SparkWaterfall</code> implementation for scoping a dataset imported from scikit-learn.</p> <pre><code>from sklearn import datasets\nimport pyspark\nfrom waterfall_logging.log import SparkWaterfall\n\ncalifornia_housing = datasets.fetch_california_housing(as_frame=True)\ncalifornia_housing_df = california_housing.frame.reset_index()\n\nspark_session = pyspark.sql.SparkSession.builder.enableHiveSupport().appName(str(__file__)).getOrCreate()\ncalifornia_housing_sdf = spark_session.createDataFrame(california_housing_df)\n\nsettings = {\n    'min_bedrooms': 1.0,\n    'max_occupants': 2.0,\n    'house_age': [10, 80],\n    'join_new_houses': True,\n}\n\nwaterfall_log = SparkWaterfall(table_name='california housing', columns=['MedInc', 'AveRooms'],  distinct_columns=['Population'])\nwaterfall_log.log(california_housing_sdf, reason='Raw california housing data', configuration_flag=None)\n\ncalifornia_housing_sdf = filter_bedrooms(california_housing_sdf, settings)\nwaterfall_log.log(california_housing_sdf, reason='Select in-scope bedrooms', configuration_flag=f\"{settings['min_bedrooms']}\")\n\nif settings['join_new_houses']:\n    virgina_housing_sdf = spark_session.createDataFrame(california_housing_df)\n    waterfall_log.log(virgina_housing_sdf, table_name='virginia housing', reason=\"Load Virginia's housing data\", configuration_flag='n/a')\n\n    california_housing_sdf = join_new_house_data(california_housing_sdf, virgina_housing_sdf)\n    waterfall_log.log(california_housing_sdf, reason='Join houses w/ 0.5 to 1.0 average bedrooms', configuration_flag=f\"{settings['join_new_houses']}\")\n\ncalifornia_housing_sdf = filter_household_members(california_housing_sdf, settings)\nwaterfall_log.log(california_housing_sdf, reason='Maximum two occupants', configuration_flag=f\"{settings['max_occupants']}\")\n\ncalifornia_housing_sdf = filter_house_age_range(california_housing_sdf, settings)\nwaterfall_log.log(california_housing_sdf, reason='House age range', configuration_flag=f\"{settings['house_age']}\")\n\nwaterfall_log.to_markdown('output/tests/california_housing_sdf.md')\n\nwaterfall_log = SparkWaterfall(table_name='california housing sdf')\nwaterfall_log.read_markdown('output/tests/california_housing_sdf.md',\n    sep='|', header=0, index_col=False, skiprows=[1], skipinitialspace=True\n)\n</code></pre>"},{"location":"tutorials/pyspark/#plotting","title":"Plotting","text":"<p>Below you can see how to plot the markdown table.</p> <pre><code>fig = waterfall_log.plot(y_col='Population', y_col_delta='\u0394 Population', x_col='Reason', drop_zero_delta=False,\n    textfont=dict(family='sans-serif', size=11),\n    connector={'line': {'color': 'rgba(0,0,0,0)'}},\n    totals={'marker': {'color': '#dee2e6', 'line': {'color': '#dee2e6', 'width': 1}}}\n)\n\nfig.update_layout(\n    autosize=True,\n    width=1000,\n    height=1000,\n    title=f'Data filtering steps',\n    xaxis=dict(title='Filtering steps'),\n    yaxis=dict(title='# of entries'),\n    showlegend=False,\n    waterfallgroupgap=0.1,\n)\n\nfig.update_traces(\n    textposition='outside',\n)\n\nfig.write_image('output/tests/california_housing_sdf.png')\n</code></pre> <p></p>"}]}